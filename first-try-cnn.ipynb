{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sample_submission.csv', 'train.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio as im\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sample_submission.csv', 'train.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os,cv2\n",
    "from IPython.display import Image\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers\n",
    "from keras import layers,models\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"../input/train/train\"\n",
    "test_dir=\"../input/test/test\"\n",
    "train=pd.read_csv('../input/train.csv')\n",
    "\n",
    "df_test=pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
       "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
       "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
       "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
       "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out dataset has 17500 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('out dataset has {} rows and {} columns'.format(train.shape[0],train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.750629\n",
       "0    0.249371\n",
       "Name: has_cactus, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['has_cactus'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in test set is 4000\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of rows in test set is %d\"%(len(os.listdir('../input/test/test'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDp59C8G648Fvr3iB9KkSHh44YIgvIA/wBZdHByT82MncOuAa6Kz+Evw6trWKU+Ob9VRQpIuNPXdwuDgsPT1xyeK8YtvEMV9eP5MiTCObyy2yRkUZypykbbTyDjjv0Fa1/8LvGGtys1rqllaOrgwibS9UEWcsCSBanPGCMHBJPLDmlSbXunXGLi7HpV/wCF/Afh2I38GuX0ka5VGUWLYZ3/AOWeJSdxbHTJOO9cxrdxbwXkaRR3EdvLxDPMg8112xZJwSCwZjjkcDoeSbA+GniC1t7zzLI6vsRLiOZIxbxxHeQ/F0I9uCYgM7cliQ3y4PNLZ6hdXc1rqqRW96iISsUqSQDCsCN8TMhzlWJ+8DxjAINuJ1xdkaNrZ6NYT6deWGm6Q8V1D5wlhSRWmbAd2GyQDcyssufvFZBJ8y5NGpa09i1s2meFLTTmkdwXt4L6FGyzFuEnQtyHOQMMDkNxT9P0GSXxVq95BeTQ295PHItvN5aQQTi63wrG7FiXVGktmTZypIyAimutls2SWfa8N9cQQxW+0HKp8/mSksfmdmcvjqFDepwJd1qOUdTA0TwLfapcyahrw3XE2yWK3vZ5wI2RGVvknlIUF3lCnJJCK5fCqBqP8PLh5oXi1DSZHjQxKr6naxAAnPDeYVyP89TSx2ItzvXdaBCSyMuFkOWz0OedxORgnNRtqV5YW0hMjCGBwSGOAGJGWAydvPvnpSg3MSg2f//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "image/jpeg": {
       "height": 250,
       "width": 250
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(os.path.join(\"../input/train/train\",train.iloc[0,0]),width=250,height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data,m,direc):\n",
    "    print('preparing data')\n",
    "    X_train=np.zeros((m,32,32,3))\n",
    "    count=0\n",
    "    for fig in data['id']:\n",
    "        img=image.load_img(os.path.join(direc,fig),target_size=(32,32,3))\n",
    "        x=image.img_to_array(img)\n",
    "        x=preprocess_input(x)\n",
    "        X_train[count]=x/255\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "    print(\"Done\")\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data\n",
      "Done\n",
      "preparing data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data=prepare_data(train,train.shape[0],train_dir)\n",
    "test=prepare_data(df_test,len(df_test),test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=data[:15001]\n",
    "train_y=train['has_cactus'][:15001]\n",
    "test_x=data[15001:]\n",
    "test_y=train['has_cactus'][15001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>has_cactus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000940378805c44108d287872b2f04ce.jpg</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  has_cactus\n",
       "0  000940378805c44108d287872b2f04ce.jpg         0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-fe95687a4b06>:43: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From <ipython-input-14-fe95687a4b06>:50: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-14-fe95687a4b06>:62: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-14-fe95687a4b06>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Epoch 0, last batch accuracy: 99.0000%, valid. accuracy: 97.9192%, valid. best loss: inf\n",
      "Epoch 1, last batch accuracy: 98.0000%, valid. accuracy: 98.8395%, valid. best loss: inf\n",
      "Epoch 2, last batch accuracy: 99.0000%, valid. accuracy: 99.1597%, valid. best loss: inf\n",
      "Epoch 3, last batch accuracy: 100.0000%, valid. accuracy: 99.3197%, valid. best loss: 0.033412\n",
      "Epoch 4, last batch accuracy: 100.0000%, valid. accuracy: 99.6399%, valid. best loss: 0.033412\n",
      "Epoch 5, last batch accuracy: 100.0000%, valid. accuracy: 99.5598%, valid. best loss: 0.033412\n",
      "Epoch 6, last batch accuracy: 100.0000%, valid. accuracy: 99.5198%, valid. best loss: 0.031530\n",
      "Epoch 7, last batch accuracy: 100.0000%, valid. accuracy: 99.2797%, valid. best loss: 0.031530\n",
      "Epoch 8, last batch accuracy: 100.0000%, valid. accuracy: 99.5598%, valid. best loss: 0.031530\n",
      "Epoch 9, last batch accuracy: 100.0000%, valid. accuracy: 99.6399%, valid. best loss: 0.028284\n",
      "Epoch 10, last batch accuracy: 100.0000%, valid. accuracy: 99.5198%, valid. best loss: 0.028284\n",
      "Epoch 11, last batch accuracy: 100.0000%, valid. accuracy: 99.7199%, valid. best loss: 0.028284\n",
      "Epoch 12, last batch accuracy: 100.0000%, valid. accuracy: 99.5998%, valid. best loss: 0.028284\n",
      "Epoch 13, last batch accuracy: 100.0000%, valid. accuracy: 99.7199%, valid. best loss: 0.025948\n",
      "Epoch 14, last batch accuracy: 100.0000%, valid. accuracy: 99.2397%, valid. best loss: 0.025948\n",
      "Epoch 15, last batch accuracy: 100.0000%, valid. accuracy: 99.6799%, valid. best loss: 0.025948\n",
      "Epoch 16, last batch accuracy: 100.0000%, valid. accuracy: 99.5998%, valid. best loss: 0.023196\n",
      "Epoch 17, last batch accuracy: 100.0000%, valid. accuracy: 99.4398%, valid. best loss: 0.023196\n",
      "Epoch 18, last batch accuracy: 100.0000%, valid. accuracy: 99.3597%, valid. best loss: 0.023196\n",
      "Epoch 19, last batch accuracy: 100.0000%, valid. accuracy: 99.6799%, valid. best loss: 0.018907\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "# Parameters for TWO convolutional layers: \n",
    "\n",
    "conv3_fmaps = 32\n",
    "conv3_ksize = 3\n",
    "conv3_stride = 1\n",
    "conv3_pad = \"SAME\"\n",
    "\n",
    "conv1_fmaps = 128\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 1\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "# Define a pooling layer\n",
    "pool3_dropout_rate = 0.25\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "# Define a fully connected layer \n",
    "n_fc1 = 128\n",
    "fc1_dropout_rate = 0.5\n",
    "\n",
    "# Output\n",
    "n_outputs = 2\n",
    "\n",
    "\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# Step 2: Set up placeholders for input data\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 32,32,3], name=\"X\")\n",
    "    y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    y_true=tf.argmax(y, dimension=1)\n",
    "    training = tf.placeholder_with_default(False, shape=[], name='training')\n",
    "\n",
    "    \n",
    "# Step 3: Set up the two convolutional layers using tf.layers.conv2d\n",
    "conv3 = tf.layers.conv2d(X, filters=conv3_fmaps, kernel_size=conv3_ksize,\n",
    "                         strides=conv3_stride, padding=conv3_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")\n",
    "conv1 = tf.layers.conv2d(conv3, filters=conv1_fmaps, kernel_size=conv1_ksize,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv3\")\n",
    "\n",
    "# Step 4: Set up the pooling layer with dropout using tf.nn.max_pool \n",
    "with tf.name_scope(\"pool3\"):\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 16 * 16])\n",
    "    pool3_flat_drop = tf.layers.dropout(pool3_flat, pool3_dropout_rate, training=training)\n",
    "\n",
    "# Step 5: Set up the fully connected layer using tf.layers.dense\n",
    "with tf.name_scope(\"fc1\"):\n",
    "    fc1 = tf.layers.dense(pool3_flat_drop, n_fc1, activation=tf.nn.relu, name=\"fc1\")\n",
    "    fc1_drop = tf.layers.dropout(fc1, fc1_dropout_rate, training=training)\n",
    "\n",
    "# Step 6: Calculate final output from the output of the fully connected layer\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(fc1, n_outputs, name=\"output\")\n",
    "    Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "    y_pred=tf.argmax(Y_proba, dimension=1)\n",
    "\n",
    "# Step 5: Define the optimizer; taking as input (learning_rate) and (loss)\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Step 6: Define the evaluation metric\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# Step 7: Initiate    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Step 8: Read in data\n",
    "\n",
    "\n",
    "# Step 9: Define some necessary functions\n",
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: tf.get_default_graph().get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    tf.get_default_session().run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# Step 10: Define training and evaluation parameters\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "iteration = 0\n",
    "\n",
    "best_loss_val = np.infty\n",
    "check_interval = 500\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "best_model_params = None \n",
    "\n",
    "# Step 11: Train and evaluate CNN with Early Stopping procedure defined at the very top\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(train_x, train_y, batch_size):\n",
    "            iteration += 1\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "            if iteration % check_interval == 0:\n",
    "                loss_val = loss.eval(feed_dict={X: test_x, y: test_y})\n",
    "                if loss_val < best_loss_val:\n",
    "                    best_loss_val = loss_val\n",
    "                    checks_since_last_progress = 0\n",
    "                    best_model_params = get_model_params()\n",
    "                else:\n",
    "                    checks_since_last_progress += 1\n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: test_x, y: test_y})\n",
    "        print(\"Epoch {}, last batch accuracy: {:.4f}%, valid. accuracy: {:.4f}%, valid. best loss: {:.6f}\".format(\n",
    "                  epoch, acc_batch * 100, acc_val * 100, best_loss_val))\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "    if best_model_params:\n",
    "        restore_model_params(best_model_params)\n",
    "    prediction= y_pred.eval(feed_dict={X: test})\n",
    "    save_path = saver.save(sess, \"./my_mnist_model\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'id':df_test['id'] })\n",
    "df['has_cactus']=prediction\n",
    "df.to_csv(\"samplesubmission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
